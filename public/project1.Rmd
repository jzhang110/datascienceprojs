---
title: 'Project 1: Exploratory Data Analysis'
author: "SDS348"
date: ''
output:
  pdf_document:
    toc: no
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
```

## Data Wrangling and Data Exploration

### Instructions
A knitted R Markdown document (as a PDF) and the raw R Markdown file (as .Rmd) should both be submitted to Canvas by 11:59pm on 3/8/2020. These two documents will be graded jointly, so they must be consistent (i.e., don’t change the R Markdown file without also updating the knitted document).

The text of the document should provide a narrative structure around your code/output. All results presented must have corresponding code. Any answers/results/plots etc. given without the corresponding R code that generated the result will not be considered. Furthermore, all code contained in your final project document must work correctly (knit early, knit often)! Please do not include any extraneous code or code which produces error messages. (Code that produces warnings is acceptable, as long as you understand what the warnings mean!)

### Find data:

Find two (!) datasets with one variable in common (e.g., dates, times, states, counties, countries, sports players), both with at least 50 observations (i.e., rows) in each. Please think carefully it makes sense to combine your datasets! When combined, the resulting/final dataset must have **at least 4 different variables (at least 3 numeric) in addition to the common variable** (i.e., five variables total).

You can have as many variables as you would like! If you found two datasets that you like but they don't have enough variables, find a third dataset with the same common variable and join all three.


### Guidelines

1. If the datasets are not tidy, you will need to reshape them so that every observation has its own row and every variable its own column. If the datasets are both already tidy, you will make them untidy with `pivot_wider()/spread()` and then tidy them again with `pivot_longer/gather()` to demonstrate your use of the functions. It's fine to wait until you have your descriptives to use these functions (e.g., you might want to pivot_wider() to rearrange the data to make your descriptive statistics easier to look at); it's fine long as you use them at least once!

    - Depending on your datasets, it might be a good idea to do this before joining. For example, if you have a dataset you like with multiple measurements per year, but you want to join by year, you could average over your numeric variables to get means/year, do counts for your categoricals to get a counts/year, etc.
    
    - If your data sets are already tidy, demonstrate the use of `pivot_longer()/gather()` and `pivot_wider()/spread()` on all or part of your data at some point in this document (e.g., after you have generated summary statistics in part 3, make a table of them wide instead of long).
    

2. Join your 2+ separate data sources into a single dataset

    - You will document the type of join that you do (left/right/inner/full), including a discussion of how many cases in each dataset were dropped (if any) and why you chose this particular join


3. Create summary statistics

    - Use *all six* core `dplyr` functions (`filter, select, arrange, group_by, mutate, summarize`) to manipulate and explore your dataset. For mutate, create a  new variable that is a function of at least one other variable, preferably using a dplyr vector function (see dplyr cheatsheet). It's totally fine to use the `_if`, `_at`, `_all` versions of mutate/summarize instead (indeed, it is encouraged if you have lots of variables)
    
    - Create summary statistics (`mean, sd, var, n, quantile, min, max, n_distinct, cor`, etc) for each of your numeric variables both overall and after grouping by one of your categorical variables (either together or one-at-a-time; if you have two categorical variables, try to include at least one statistic based on a grouping of two categorical variables simultaneously). If you do not have any categorical variables, create one using mutate (e.g., with `case_when` or `ifelse`) to satisfy the `group_by` requirements above. Ideally, you will find a way to show these summary statistics in an easy-to-read table (e.g., by reshaping). (You might explore the kable package for making pretty tables!) If you have lots of numeric variables, or your categorical variables have too many categories, just pick a few (either numeric variables or categories of a categorical variable) and summarize based on those. It would be a good idea to show a correlation matrix for your numeric variables!
 
4. Make visualizations (three plots)

    -  Make a correlation heatmap of your numeric variables
    -  Create at least two additional plots of your choice with ggplot that highlight some of the more interesting findings that your descriptive statistics have turned up.
    - Each plot (besides the heatmap) should have at least three variables mapped to separate aesthetics
    - At least one plot should include `stat="summary"`
    - Each plot should include a supporting paragraph describing the relationships that are being visualized and any trends that are apparent
        - It is fine to include more, but limit yourself to 4. Plots should avoid being redundant! Four bad plots will get a lower grade than two good plots, all else being equal.
    - Make them pretty! Use correct labels, etc.
    
    
5. Perform k-means/PAM clustering or PCA on (at least) your numeric variables.

    - Include all steps as we discuss in class, including a visualization.

    - If you don't have at least 3 numeric variables, or you want to cluster based on categorical variables too, convert them to factors in R, generate Gower's dissimilarity matrix on the data, and do PAM clustering on the dissimilarities.
    
    - Show how you chose the final number of clusters/principal components 
    
    - Interpret the final clusters/principal components 

    - For every step, document what your code does (in words) and what you see in the data!     

<P style="page-break-before: always">
\newpage
    
### Rubric

Prerequisite: Finding appropriate data from at least two sources per the instructions above: Failure to do this will result in a 0! You will submit a .Rmd file and a knitted document (pdf).

#### 0. Introduction (4  pts)

- Write a narrative introductory paragraph or two describing the datasets you have chosen, the variables they contain, how they were acquired, and why they are interesting to you. Expand on potential associations you may expect, if any.

For the exploratory analysis project, I chose to combine two datasets by the country variable. The first dataset I chose was about average plastic waste per country that was measured in kg/person/day. The first dataset was obtained from GitHub’s tidytuesday project. In addition to containing average plastic waste, the first dataset also included the country’s three letter code, the GDP per capital of each country in the data set, year, and total population of the country. I chose this dataset because I was very interested on the trend in plastic waste in countries around the world. I was hoping to compare the different countries to see how plastic waste relates to other aspects of a country such as GDP and population. The second dataset I chose was a dataset about the average life expectancy (in years) in various countries. The second dataset was also obtained from GitHub’s tidetuesday project. In addition to containing life expectancy for all of the countries in the dataset, the dataset also included the various countries’ three letter codes along with the year that the life expectancy data was measured. I chose the second dataset because I wanted to compare the different countries to see if GDP per capita and population have any correlation with life expectancy. I expected that there was an association between GDP and life expectancy along with GDP and plastic waste. 

#### 1. Tidying: Rearranging Wide/Long (8 pts)

```{R}
plastic <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-21/per-capita-plastic-waste-vs-gdp-per-capita.csv")
lifeexpraw <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-07-03/week14_global_life_expectancy.csv")
library(tidyverse)
plasticwasteraw<-plastic%>%na.omit()
plasticwaste<-plasticwasteraw[ -c(3) ]
lifeexp<-lifeexpraw%>%na.omit()
names(plasticwaste)[1]<- "country"
names(plasticwaste)[3]<- "PlasticWaste(kg/person/day)"
names(plasticwaste)[4]<-"GDP(per capita)"
names(plasticwaste)[5]<-"Population"
names(lifeexp)[4]<- "life"
head(plasticwaste)
head(lifeexp)
untidyplasticwaste<-plasticwaste%>%pivot_wider(names_from = Code, values_from = 'PlasticWaste(kg/person/day)')
tidyplasticwaste<-untidyplasticwaste%>%pivot_longer(2:148,names_to="Code", values_to = "waste",values_drop_na = T)
head(untidyplasticwaste)
head(tidyplasticwaste)

```

- Tidy the datasets (using the `tidyr` functions `pivot_longer`/`gather` and/or `pivot_wider`/`spread`) 
- If you data sets are already tidy, be sure to use those functions somewhere else in your project
- Document the process (describe in words what was done per the instructions)

Each of my datasets were already tidy to begin with other than needing to delete extraneous columns and omitting NAs. I demonstrated my tidying skills by first using pivot wider on the plastic dataset to reshape the dataset from long to wide. The process was done by first taking out a few columns and then using pivot_wider with names_from = Code and values_from = 'PlasticWaste(kg/person/day)' in order to make an extra-long dataset (a lot of columns). The extra long dataset was then named to be untidyplasticwaste. Next, pivot_longer was used to tidy the dataset into tidyplasticwaste.
    
#### 2. Joining/Merging (8 pts)

```{R}
lifeandwaste <- lifeexp %>%inner_join(plasticwaste, by="country")
lifeandwaste<-lifeandwaste[ -c(5) ]
head(lifeandwaste)
```
- Join your datasets into one using a `dplyr` join function
- If you have multiple observations on the joining variable in either dataset, fix this by collapsing via summarize
- Discuss the process in words, including why you chose the join you did
- Discuss which cases were dropped, if any, and potential problems with this

I chose inner join to join my two datasets because I wanted to focus on primarily the countries that are involved in both of the datasets so I can get all the data that was available about a specific country from both datasets. The Column “Code” was deleted form the joined dataset as the data was inner joined by “country” and it left the joined dataset with two columns for the three letter codes that were in both of the original datasets. The joined dataset did not contain any NAs as the NAs were already omitted in the first part of the project. Due to the omitting of the NAs, 22059 cases were dropped from the plastics dataset, and 2508 cases were dropped form the life expectancy dataset. Potential problems with the drop of these cases is that the sample size in the plastic dataset just significantly dropped. This means that less data are available after the joining process to be analyzed. 
 
#### 3. Wrangling (40 pts)

```{R}
names(lifeandwaste)[4]<-"lifespan(years)"
gdpdata <-lifeandwaste%>% mutate(GDP=`GDP(per capita)`*Population)
head(gdpdata)
gdpdata%>%arrange(`lifespan(years)`)
gdpdata%>%arrange(desc(`lifespan(years)`))
gdpdata%>%arrange(`GDP`)
gdpdata%>%arrange(desc(GDP))
gdpdata%>% summarize(mean(`GDP(per capita)`))
gdpdata%>% summarize(sd(`GDP(per capita)`))
gdpdata%>% summarize(median(`GDP(per capita)`))
gdpdata%>%summarize(mean(`lifespan(years)`))
gdpdata%>%summarize(sd(`lifespan(years)`))
gdpdata%>%summarize(median(`lifespan(years)`))
gdpdata%>%summarize(mean(`PlasticWaste(kg/person/day)`))
gdpdata%>%summarize(sd(`PlasticWaste(kg/person/day)`))
gdpdata%>%summarize(median(`PlasticWaste(kg/person/day)`))
gdpdata%>%summarize(mean(`Population`))
gdpdata%>%summarize(median(`Population`))
gdpdata%>%summarize(sd(`Population`))
gdpdata%>%summarize(mean(`GDP`))
gdpdata%>%summarize(median(`GDP`))
gdpdata%>%summarize(sd(`GDP`))
livelong<-gdpdata%>%filter(`lifespan(years)`>62.732)
n_distinct(livelong$country)
liveshort<-gdpdata%>%filter(`lifespan(years)`<62.732)
n_distinct(liveshort$country)
moremoney<-gdpdata%>%filter(`GDP(per capita)`>15715.97)
n_distinct(moremoney$country)
lessmoney<-gdpdata%>%filter(`GDP(per capita)`<15715.97)
n_distinct(lessmoney$country)
sum1<-gdpdata %>%group_by(country,year)%>%summarize(meanlife=mean(`lifespan(years)`))
sum1%>%arrange(desc(meanlife))%>%head()
sum2<-gdpdata %>%group_by(`country`)%>%summarize(avggdppercap=mean(`GDP(per capita)`))
sum2%>%arrange(desc(avggdppercap))%>%head()
sum3<-gdpdata %>%group_by(`country`)%>%summarize(avgwaste=mean(`PlasticWaste(kg/person/day)`))
sum3%>%arrange(desc(avgwaste))%>%head()
sum4<-gdpdata %>%group_by(`country`)%>%summarize(avgpop=mean(`Population`))
sum4%>%arrange(desc(avgpop))%>%head()
sum5<-gdpdata %>%group_by(`country`)%>%summarize(avggdp=mean(GDP))
sum5%>%arrange(desc(avggdp))%>%head()
proj1data <-gdpdata%>%select(`country`, `lifespan(years)`, `GDP(per capita)`)
head(proj1data)
```

- Use all six core `dplyr` functions in the service of generating summary statistics (18 pts)
    - Use mutate to generate a variable that is a function of at least one other variable

- Compute at least 10 different summary statistics using summarize and summarize with group_by (18 pts)
    - At least 2 of these should group by a categorical variable. Create one by dichotomizing a numeric if necessary
    - If applicable, at least 1 of these 5 should group by two categorical variables
    - Strongly encouraged to create a correlation matrix with `cor()` on your numeric variables

- Summarize/discuss all results in no more than two paragraphs (4 pts)

Next, summary statistics were generated. First, I created a new column that was named “GDP” and it was created by multiplying the values from the “GDP(per capita)” column by the “Population” column. After the mutate function, the arrange function was used to determine the countries with the highest life expectancy. It was found that Hongkong, Macao, and Japan were the top three in life expectancy. In addition, the arrange function was also used to determine the country with the highest GDP in the dataset. The country with the highest GDP was determined to be the United States. After that, the summarize function was used and the mean GDP per capita of all the countries in the joined dataset was determined to be $22109.69 with a standard deviation of 20235.37 and a median of $15715.97. The mean life expectancy of all the countries in the joined dataset was determined as 60.25015 years with a standard deviation of 13.27531 and a median of 62.732 years. The mean plastic was determined as 0.1932676 kg/person/day for the countries in the joined dataset. The standard deviation for plastic waste was 0.3000896 along with a median of 0.144 kg/person/day. The population mean for the countries in the joined dataset was determined as 46516861 people with a standard deviation of 153287283 and a median of 9379687 people. The mean GDP for all the countries in the dataset was determined to be $761813638316 with a standard deviation of 2020639381785 and a median of $127181583800. Using the filter function, it was determined that there were 122 countries that had their life expectancy above the median life expectancy of 62.732 years and 134 countries below the median life expectancy. Again using the filter function, it was determined that there were 60 countries that had their GDP per capita above the median of $15715.97 and 78 countries that had their GDP per capita less than the median. Next, the group_by function was then used to group the dataset by the country and year variable in addition to giving a mean life expectancy of the specific country and year group. This showed that in the year 2015, Hongkong had the highest life expectancy of 83.801 years. Using the group_by function again, this time it was grouped by country and a summarize function was used to find the mean GDP per capita for each country. The country with the highest average GDP per capital regardless of the year was Qatar with an average GDP per capita of $125140.84. Grouped by country and summarized by average plastic waste, the country with the highest plastic waste regardless of everything else was Trinidad and Tobago with an average of 3.6 kg/person/day. Grouped by country and summarized by average population, the country with the highest population regardless of everything else was China with an average population of 1341335152 people. Grouped by country and summarized by average GDP, the country with the highest average GDP was the United States with an average GDP of $15324952571640.

#### 4. Visualizing (30 pts)
- Create a correlation heatmap of your numeric variables
```{R}
library(ggplot2)
df<-gdpdata%>%na.omit%>%select_if(is.numeric)
cor(df)
corrmatrix<-cor(df)%>%as.data.frame%>%
rownames_to_column%>%
pivot_longer(-1,names_to="name",values_to="correlation")
head(corrmatrix)
corrmatrix%>%ggplot(aes(rowname,name,fill=correlation))+geom_tile()+scale_fill_gradient2(low="red",mid="white",high="blue")+geom_text(aes(label=round(correlation,2)),color = "black", size = 4)+theme(axis.text.x = element_text(angle = 90, hjust = 1))+coord_fixed() 
```
- Create two effective, polished plots with ggplot

    - Each plot should map 3+ variables to aesthetics 
    - Each plot should have a title and clean labeling for all mappings
    - Change at least one default theme element and color for at least one mapping per plot
    - For at least one plot, add more tick marks (x, y, or both) than are given by default
    - For at least one plot, use the stat="summary" function
    - Supporting paragraph or two (for each plot) describing the relationships/trends that are apparent
```{R}
gdpdataplot1<-gdpdata%>%arrange(desc(`lifespan(years)`))%>%filter(`lifespan(years)`>82.5)
ggplot(gdpdataplot1, aes(x = country, y = `lifespan(years)`, fill=code))+scale_y_continuous(breaks=c(10,20,30,40,50,60,70,80,90))+geom_bar(stat="summary",fun.y="mean",position="dodge", color="darkred")+geom_errorbar(stat="summary",width=.5)+theme(axis.text.x = element_text(angle=45, hjust=1), legend.position="top")+ggtitle("Top eight countries with high life expectancies")+ylab("life expectancy in years")

gdpplot2<-ggplot(data=gdpdataplot1, aes(x= `GDP(per capita)`, y=Population, color=country)) + geom_point(size=2, alpha=0.85)
gdpplot2+ ggtitle("GDP per Capita by Population for top 8 countries with high life expectancy")+ ylab("GDP per capita in dollars")+ xlab("Population")+theme_minimal()+scale_y_continuous(breaks=c(25000000,50000000,100000000,125000000))


```

The first plot described the life expectancy in years by country. The box plot clearly showed the top 8 countries with the highest life expectancies (Australia, Hongkong, Iceland, Italy, Japan, Macao, Singapore, and Spain) along with their individual average life expectancies. The color of each of the bars indicates the three letter codes of the country as shown by the legend above the bar plot. The trend that was noticed in this plot was that the top 8 countries with the highest life expectancies all have an average life expectancy a or around 85 years. The tick marks of the bar plot were changed from the default to give a more accurate description of the ages.

The second plot illustrated the GDP per capita by the Population for the top 8 countries with the highest life expectancies. The tick marks of the graph were modified, and the theme was set to a minimal to give a more accurate and clear visualization. The overall trend that was shown to be apparent was that as the population size of the country increases, the GDP per capita of the country drastically decreases.

#### 5. Dimensionality Reduction (20 pts) 


- Either k-means/PAM clustering or PCA (inclusive "or") should be performed on at least three numeric variables in your dataset

    - All relevant steps discussed in class 
    - A visualization of the clusters or the first few principal components (using ggplot2)
    - Supporting paragraph or two describing results found 
    
```{R}
gdpdata2<-gdpdata%>%select(-"country",-"year",-"PlasticWaste(kg/person/day)",-"code",-"GDP",-"Population")%>%glimpse()
gdpdata3<-gdpdata%>%select(-"country",-"year", -"code",-"Population",-"GDP",-"GDP(per capita)")%>%glimpse()
gdpdata4<-gdpdata%>%select(-"country",-"year", -"code",-"Population",-"GDP",-"lifespan(years)")%>%glimpse()

#GDP Per Capita and Life Expectancy
wss<-vector()
for(i in 1:10){
temp<-gdpdata2%>%dplyr::select(`lifespan(years)`,`GDP(per capita)`)%>%kmeans(.,i)
wss[i]<-temp$tot.withinss%>%glimpse()
}
ggplot()+geom_point(aes(x=1:10,y=wss))+geom_path(aes(x=1:10,y=wss))+
xlab("clusters")+scale_x_continuous(breaks=1:10)
kmeans1 <- kmeans(gdpdata2, 2)
kmeans1
kmeanscluster<-gdpdata2%>%mutate(cluster=as.factor(kmeans1$cluster))%>%glimpse
kmeanscluster%>%ggplot(aes(`lifespan(years)`,`GDP(per capita)`,color=cluster))+geom_point()
kmeans1$betweenss
kmeans1$size
kmeans1$tot.withinss
kmeans1$centers

#Life Expectancy and Plastic Waste
wss2<-vector()
for(i in 1:10){
temp3<-gdpdata3%>%dplyr::select(`lifespan(years)`,`PlasticWaste(kg/person/day)`)%>%kmeans(.,i)
wss2[i]<-temp3$tot.withinss%>%glimpse()
}
ggplot()+geom_point(aes(x=1:10,y=wss2))+geom_path(aes(x=1:10,y=wss2))+
xlab("clusters")+scale_x_continuous(breaks=1:10)
kmeans2<-kmeans(gdpdata3, 2)
kmeans2
kmeans2$betweenss
kmeans2$tot.withinss
kmeans2$size
kmeans2$centers
kmeanscluster2<-gdpdata3%>%mutate(cluster=as.factor(kmeans2$cluster))%>%glimpse
kmeanscluster2%>%ggplot(aes(`lifespan(years)`,`PlasticWaste(kg/person/day)`,color=cluster))+geom_point()

#GDP per capita and Plastic Waste
wss3<-vector()
for(i in 1:10){
temp4<-gdpdata4%>%dplyr::select(`GDP(per capita)`,`PlasticWaste(kg/person/day)`)%>%kmeans(.,i)
wss3[i]<-temp4$tot.withinss%>%glimpse()
}
ggplot()+geom_point(aes(x=1:10,y=wss3))+geom_path(aes(x=1:10,y=wss3))+
xlab("clusters")+scale_x_continuous(breaks=1:10)
kmeans3<-kmeans(gdpdata4, 2)
kmeans3
kmeans3$tot.withinss
kmeans3$centers
kmeans3$size
kmeans3$betweenss
kmeanscluster3<-gdpdata4%>%mutate(cluster=as.factor(kmeans3$cluster))
kmeanscluster3%>%ggplot(aes(`GDP(per capita)`,`PlasticWaste(kg/person/day)`,color=cluster))+geom_point()

```

For my data, a k-means cluster for multiple variables was performed on my joined dataset. The first k-means cluster was for GDP per capita and life expectancy. The data was prepped by removing all columns except for the GDP per capita and life expectancy columns. This new dataset was named to be gdpdata2. Next, the number of clusters were determined by the observation of the plot made for clusters vs. wss. The number of clusters were determined by looking at the elbow of the graph that was generated (2 was determined to be the cluster). Next, using k-means cluster, a plot was made using ggplot. Two groups are clearly visible, and from this plot generated it showed that GDP per capita didn’t really show any relationship with life expectancy as countries with low GDP per capita and high GDP per capita all can have high life expectancies. The two clusters were separated by high and low GDp per capita. The betweenss was 3042321025004 and the tot.withinss was 1.422546e+12. The cluster size was 4085 and 6802, with the group having higher GDP per capita containing 6802 countries. 

Next, a second k-means cluster for life expectancy and plastic waste was performed. The data was prepped the same way as the first cluster plot generated (dropped all columns except for the columns of interest). The number of clusters to use was determined by the plot of clusters vs wss that was generated. The cluster was again determined to be 2. The cluster was plotted again, and 2 groups were clearly observed. The 2 groups were shown to had been separated based on high and low life expectancy. The two groups were shown to have similar plastic wastes and no relationship was observed between plastic waste and life expectancy. The cluster sizes were 6650 and 4255 with 6650 being cluster 1. In addition, the betweenss was determined to be 1415487 and the tot.withinss was 507147.2.

Lastly, the third k-means cluster was for GDP per capita and plastic waste. The data was prepped the same way as the first two cluster plots generated (dropped all columns except for the columns of interest). The number of clusters was again determined to be 2 by looking at the clusters vs wss plot generated. Then the clusters were plotted, and 2 distinct groups were visualized. The clusters were shown to have been separated based on high and low GDP per capita. There was no clear relationship observed between GDP per capita and plastic waste. The size of the two clusters were 6820 and 4085 with 6820 as cluster 1. The betweenss was 3.042321e+12 and the tot.withinss was 1.422544e+12. 

#### 6. Neatness!

- Your project should not knit to more than 30 or so pages. You will lose points if you print out your entire dataset, etc. If you start your project in a fresh .Rmd file, you are advised to paste the set-up code from this document (lines 14-17) at the top of it: this will automatically truncate if you accidentally print out a huge dataset, etc. Imagine this is a polished report you are giving to your PI or boss to summarize your work researching a topic.


### Where do I find data?

OK, brace yourself!

You can choose ANY datasets you want that meet the above criteria for variables and observations. I'm just sitting here but off the top of my head, if you are into amusement parks, you could look at amusement-park variables, including ticket sales per day etc.; then you could join this by date in weather data. If you are interested in Game of Thrones, you could look at how the frequency of mentions of character names (plus other character variables) and the frequency of baby names in the USA...You could even take your old Biostats data and merge in new data (e.g., based on a Google forms timestamp).

You could engage in some "me-search": You can [request your Spotify data](https://support.spotify.com/ca-en/article/data-rights-and-privacy-settings/) or [download Netflix viewing activity](https://help.netflix.com/en/node/101917), Amazon purchase history, etc. You can use your Google Fit/Fitbit/Apple watch data, etc. These can be combined (e.g., with each other, with other data sources).

You can make it as serious as you want, or not, but keep in mind that you will be incorporating this project into a portfolio webpage for your final in this course, so choose something that really reflects who you are, or something that you feel will advance you in the direction you hope to move career-wise, or something that you think is really neat. On the flip side, regardless of what you pick, you will be performing all the same tasks, so it doesn't end up being that big of a deal.

If you are totally clueless and have no direction at all, log into the server and type 

```{R eval=F}
data(package = .packages(all.available = TRUE))
```

This will print out a list of **ALL datasets in ALL packages** installed on the server (a ton)! Scroll until your eyes bleed! Actually, do not scroll that much... To start with something more manageable, just run the command on your own computer, or just run `data()` to bring up the datasets in your current environment. To read more about a dataset, do `?packagename::datasetname`. 

If it is easier for you, and in case you don't have many packages installed, a list of R datasets from a few common packages (also downloadable in CSV format) is given at the following website: https://vincentarelbundock.github.io/Rdatasets/datasets.html (including types/numbers of variables in each)

- A good package to download for fun/relevant data is `fivethiryeight`. Just run `install.packages("fivethirtyeight"), load the packages with `library(fivethirtyeight)`, run `data()`, and then scroll down to view the datasets. Here is an online list of all 127 datasets (with links to the 538 articles). Lots of sports, politics, current events, etc: https://cran.r-project.org/web/packages/fivethirtyeight/vignettes/fivethirtyeight.html

- If you have already started to specialize (e.g., ecology, epidemiology) you might look at discipline-specific R packages (vegan, epi, respectively). We will be using some tools from these packages later in the course, but they come with lots of data too, which you can explore according to the directions above

- However, you *emphatically DO NOT* have to use datasets available via R packages! In fact, I would much prefer it if you found the data from completely separate sources and brought them together (a much more realistic experience in the real world)! You can even reuse data from your SDS328M project, provided it shares a variable in common with other data which allows you to  merge the two together (e.g., if you still had the timestamp, you could look up the weather that day: https://www.wunderground.com/history/). If you work in a research lab or have access to old data, you could potentially merge it with new data from your lab!

- Here is a curated list of interesting datasets (read-only spreadsheet format): https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit

- Here is another great compilation of datasets: https://github.com/rfordatascience/tidytuesday

- Here is the UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/index.php

    - See also https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research#Biological_data

- Here is another good general place to look: https://www.kaggle.com/datasets

- To help narrow your search down or to see interesting variable ideas, check out https://www.tylervigen.com/spurious-correlations. This is the spurious correlations website, and it is fun, but if you look at the bottom of each plot you will see sources for the data. This is a good place to find very general data (or at least get a sense of where you can scrape data together from)!

- If you are interested in medical data, check out www.countyhealthrankings.org

- If you are interested in scraping UT data, the university makes *loads* of data public (e.g., beyond just professor CVs and syllabi). Check out all the data that is available in the statistical handbooks: https://reports.utexas.edu/statistical-handbook

##### Broader data sources:

[Data.gov](www.data.gov) 186,000+ datasets!

[Social Explorer](Social Explorer) is a nice interface to Census and American Community Survey data (more user-friendly than the government sites). May need to sign up for a free trial.

[U.S. Bureau of Labor Statistics](www.bls.gov)

[U.S. Census Bureau](www.census.gov)

[Gapminder](www.gapminder.org/data), data about the world.

...





